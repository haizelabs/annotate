{
  "annotation_ux_state": {
    "granularity": "trace",
    "annotation_spec": {
      "type": "categorical",
      "categories": ["pass", "fail", "neutral", "idk"]
    }
  },
  "judge_config": {
    "label_type_reference": {
      "type": "categorical",
      "categories": ["pass", "fail", "neutral", "idk"]
    },
    "model_name": "gpt-4o-mini",
    "rubric": "Evaluate the overall quality of the LLM-generated essay evaluation. Consider accuracy of scoring, clarity of rationale, and adherence to the rubric criteria.",
    "judge_input": "trace"
  }
}
